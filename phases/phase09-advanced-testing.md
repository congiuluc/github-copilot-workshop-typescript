# üß™ Phase 09: Advanced Testing
[![‚¨ÖÔ∏è Back to Workshop Home](https://img.shields.io/badge/‚¨ÖÔ∏è-Back%20to%20Workshop%20Home-blue?style=flat-square)](../README.md) [![‚¨ÖÔ∏è Previous: Phase 08](https://img.shields.io/badge/‚¨ÖÔ∏è-Previous%3A%20Phase%2008-lightgrey?style=flat-square)](phase08-ai-commit-messages.md) [![‚û°Ô∏è Next: Phase 10](https://img.shields.io/badge/‚û°Ô∏è-Next%3A%20Phase%2010-lightgrey?style=flat-square)](phase10-documentation-diagrams.md)

**Feature Focus**: Test generation, optimization, and comprehensive test suite development

## üéØ Objective

Master GitHub Copilot's testing capabilities to create comprehensive test suites, including unit tests, integration tests, performance tests, and edge case coverage for your TypeScript application.

## üìñ About Copilot Testing Features

Copilot Testing provides:
- **Automated test generation** for existing code
- **Test case discovery** including edge cases
- **Mock and stub creation** for dependencies
- **Test data generation** for various scenarios
- **Testing best practices** implementation
- **Performance and load testing** suggestions

**Documentation**: [GitHub Copilot for Testing](https://docs.github.com/en/copilot/using-github-copilot/asking-github-copilot-questions-in-your-ide)

## üèóÔ∏è What We'll Test

In this phase, we'll create:
- Unit tests for all models and services
- Integration tests for complete workflows
- Mock implementations for external dependencies
- Performance tests for critical operations
- Edge case and error condition tests
- End-to-end API testing scenarios

## üìã Step-by-Step Instructions

### Step 1: Setup Testing Infrastructure

1. **Install testing dependencies**:
   ```bash
   npm install -D jest @types/jest ts-jest
   npm install -D supertest @types/supertest
   npm install -D @faker-js/faker
   npm install -D jest-extended
   ```

2. **Create Jest configuration** `jest.config.js`:
   ```javascript
   module.exports = {
     preset: 'ts-jest',
     testEnvironment: 'node',
     roots: ['<rootDir>/src'],
     testMatch: ['**/__tests__/**/*.ts', '**/?(*.)+(spec|test).ts'],
     transform: {
       '^.+\\.ts$': 'ts-jest',
     },
     collectCoverageFrom: [
       'src/**/*.ts',
       '!src/**/*.d.ts',
       '!src/**/*.test.ts',
       '!src/**/*.spec.ts',
     ],
     coverageDirectory: 'coverage',
     coverageReporters: ['text', 'lcov', 'html'],
     setupFilesAfterEnv: ['jest-extended/all'],
   };
   ```

3. **Update package.json scripts**:
   ```json
   {
     "scripts": {
       "test": "jest",
       "test:watch": "jest --watch",
       "test:coverage": "jest --coverage",
       "test:ci": "jest --ci --coverage --watchAll=false"
     }
   }
   ```

### Step 2: Generate Unit Tests with /tests Command

1. **Open your TaskService** from previous phases

2. **Select the entire TaskService class**

3. **Use Copilot Chat with /tests command**:
   ```
   /tests Generate comprehensive unit tests for this TaskService class including happy path, error cases, edge cases, and mock dependencies. Include tests for all public methods with proper assertions and test data setup.
   ```

4. **Create** `src/__tests__/services/TaskService.test.ts` with generated tests:
   ```typescript
   import { TaskService } from '../../services/TaskService';
   import { TaskRepository } from '../../interfaces/TaskRepository';
   import { UserService } from '../../services/UserService';
   import { Logger } from '../../services/Logger';
   import { Task } from '../../models/Task';
   import { TaskStatus, TaskPriority } from '../../types/enums';
   import { faker } from '@faker-js/faker';

   // Mock implementations generated by Copilot
   const mockTaskRepository: jest.Mocked<TaskRepository> = {
     findById: jest.fn(),
     findByUserId: jest.fn(),
     create: jest.fn(),
     update: jest.fn(),
     delete: jest.fn(),
     findAll: jest.fn(),
   };

   const mockUserService: jest.Mocked<UserService> = {
     findById: jest.fn(),
     validatePermissions: jest.fn(),
   };

   const mockLogger: jest.Mocked<Logger> = {
     info: jest.fn(),
     error: jest.fn(),
     warn: jest.fn(),
     debug: jest.fn(),
   };

   describe('TaskService', () => {
     let taskService: TaskService;

     beforeEach(() => {
       taskService = new TaskService(mockTaskRepository, mockUserService, mockLogger);
       jest.clearAllMocks();
     });

     describe('createTask', () => {
       it('should create a task successfully with valid data', async () => {
         // Copilot-generated test implementation
         const taskData = {
           title: faker.lorem.sentence(),
           description: faker.lorem.paragraph(),
           assignedUserId: faker.string.uuid(),
           priority: TaskPriority.HIGH,
         };

         const expectedTask = new Task(taskData.title, taskData.description, taskData.assignedUserId);
         mockTaskRepository.create.mockResolvedValue(expectedTask);
         mockUserService.findById.mockResolvedValue({ id: taskData.assignedUserId } as any);

         const result = await taskService.createTask(taskData);

         expect(result).toBe(expectedTask);
         expect(mockTaskRepository.create).toHaveBeenCalledWith(taskData);
         expect(mockLogger.info).toHaveBeenCalledWith(
           'Task created successfully',
           expect.objectContaining({ taskId: expectedTask.id })
         );
       });

       it('should throw ValidationError for invalid title', async () => {
         const taskData = {
           title: '', // Invalid empty title
           description: faker.lorem.paragraph(),
           assignedUserId: faker.string.uuid(),
         };

         await expect(taskService.createTask(taskData))
           .rejects
           .toThrow('Task title is required');

         expect(mockTaskRepository.create).not.toHaveBeenCalled();
       });

       // More comprehensive tests generated by Copilot...
     });
   });
   ```

### Step 3: Generate Mock Data and Test Utilities

1. **Use Copilot Chat to generate test utilities**:
   ```
   Create a comprehensive test data factory for generating realistic test data for Task, User, and Project models. Include builders for different scenarios like overdue tasks, high-priority tasks, and completed projects.
   ```

2. **Create** `src/__tests__/utils/testDataFactory.ts`:
   ```typescript
   import { faker } from '@faker-js/faker';
   import { Task } from '../../models/Task';
   import { User } from '../../models/User';
   import { Project } from '../../models/Project';
   import { TaskStatus, TaskPriority } from '../../types/enums';

   export class TestDataFactory {
     static createTask(overrides: Partial<Task> = {}): Task {
       const defaultTask = {
         id: faker.string.uuid(),
         title: faker.lorem.sentence({ min: 3, max: 8 }),
         description: faker.lorem.paragraph(),
         status: faker.helpers.enumValue(TaskStatus),
         priority: faker.helpers.enumValue(TaskPriority),
         createdAt: faker.date.past(),
         updatedAt: faker.date.recent(),
         assignedUserId: faker.string.uuid(),
         ...overrides
       };
       
       return Object.assign(new Task(), defaultTask);
     }

     static createOverdueTask(): Task {
       return this.createTask({
         status: TaskStatus.IN_PROGRESS,
         dueDate: faker.date.past({ days: 10 }),
         priority: TaskPriority.HIGH
       });
     }

     static createHighPriorityTask(): Task {
       return this.createTask({
         priority: TaskPriority.URGENT,
         dueDate: faker.date.soon({ days: 3 }),
         status: TaskStatus.PENDING
       });
     }

     // More specialized builders...
   }
   ```

### Step 4: Generate Integration Tests

1. **Use Copilot to create integration tests**:
   ```
   /tests Create integration tests that test the complete workflow from TaskController through TaskService to TaskRepository. Include realistic scenarios like creating tasks, assigning them, updating status, and handling conflicts.
   ```

2. **Create** `src/__tests__/integration/taskWorkflow.integration.test.ts`:
   ```typescript
   import request from 'supertest';
   import { Application } from 'express';
   import { TestDataFactory } from '../utils/testDataFactory';
   import { setupTestApp } from '../utils/testApp';

   describe('Task Workflow Integration Tests', () => {
     let app: Application;

     beforeAll(async () => {
       app = await setupTestApp();
     });

     describe('Complete Task Lifecycle', () => {
       it('should handle complete task creation and assignment workflow', async () => {
         // Create user first
         const userData = TestDataFactory.createUserData();
         const userResponse = await request(app)
           .post('/api/users')
           .send(userData)
           .expect(201);

         const user = userResponse.body;

         // Create project
         const projectData = TestDataFactory.createProjectData();
         const projectResponse = await request(app)
           .post('/api/projects')
           .send(projectData)
           .expect(201);

         const project = projectResponse.body;

         // Create task
         const taskData = {
           ...TestDataFactory.createTaskData(),
           assignedUserId: user.id,
           projectId: project.id
         };

         const taskResponse = await request(app)
           .post('/api/tasks')
           .send(taskData)
           .expect(201);

         const task = taskResponse.body;
         expect(task.id).toBeDefined();
         expect(task.status).toBe(TaskStatus.PENDING);

         // Update task status
         await request(app)
           .patch(`/api/tasks/${task.id}/status`)
           .send({ status: TaskStatus.IN_PROGRESS })
           .expect(200);

         // Complete task
         await request(app)
           .patch(`/api/tasks/${task.id}/status`)
           .send({ status: TaskStatus.COMPLETED })
           .expect(200);

         // Verify final state
         const finalTaskResponse = await request(app)
           .get(`/api/tasks/${task.id}`)
           .expect(200);

         expect(finalTaskResponse.body.status).toBe(TaskStatus.COMPLETED);
       });
     });
   });
   ```

### Step 5: Generate Performance Tests

1. **Create performance test utilities**:
   ```
   Create performance tests for TaskService operations including bulk task creation, large dataset queries, and concurrent operations. Include memory usage monitoring and execution time assertions.
   ```

2. **Create** `src/__tests__/performance/TaskService.performance.test.ts`:
   ```typescript
   import { TaskService } from '../../services/TaskService';
   import { TestDataFactory } from '../utils/testDataFactory';
   import { performance } from 'perf_hooks';

   describe('TaskService Performance Tests', () => {
     let taskService: TaskService;

     beforeEach(() => {
       // Setup with in-memory repository for consistent performance testing
       taskService = setupPerformanceTestService();
     });

     describe('Bulk Operations', () => {
       it('should create 1000 tasks within acceptable time limits', async () => {
         const taskDataArray = Array.from({ length: 1000 }, () => 
           TestDataFactory.createTaskData()
         );

         const startTime = performance.now();
         
         const promises = taskDataArray.map(data => taskService.createTask(data));
         await Promise.all(promises);
         
         const endTime = performance.now();
         const duration = endTime - startTime;

         // Should complete within 5 seconds
         expect(duration).toBeLessThan(5000);
         
         // Memory usage should be reasonable
         const memoryUsage = process.memoryUsage();
         expect(memoryUsage.heapUsed).toBeLessThan(100 * 1024 * 1024); // 100MB limit
       });

       it('should handle concurrent task updates without race conditions', async () => {
         // Create initial task
         const task = await taskService.createTask(TestDataFactory.createTaskData());

         // Attempt concurrent updates
         const concurrentUpdates = Array.from({ length: 50 }, (_, index) => 
           taskService.updateTask(task.id, { 
             title: `Updated Title ${index}` 
           })
         );

         const results = await Promise.allSettled(concurrentUpdates);
         
         // Some should succeed, others should fail gracefully
         const successful = results.filter(r => r.status === 'fulfilled');
         const failed = results.filter(r => r.status === 'rejected');
         
         expect(successful.length).toBeGreaterThan(0);
         expect(failed.length).toBeGreaterThan(0);
         
         // No corrupted data
         const finalTask = await taskService.getTask(task.id);
         expect(finalTask.title).toMatch(/^Updated Title \d+$/);
       });
     });
   });
   ```

### Step 6: Generate Edge Case Tests

1. **Ask Copilot for edge case testing**:
   ```
   Generate comprehensive edge case tests for TaskService including null inputs, boundary values, malformed data, network failures, and race conditions. Cover all error scenarios and exceptional cases.
   ```

2. **Create** `src/__tests__/edge-cases/TaskService.edge.test.ts`:
   ```typescript
   describe('TaskService Edge Cases', () => {
     let taskService: TaskService;

     beforeEach(() => {
       taskService = setupTestTaskService();
     });

     describe('Input Validation Edge Cases', () => {
       it('should handle extremely long task titles gracefully', async () => {
         const longTitle = 'A'.repeat(10000); // 10k characters
         
         const taskData = {
           ...TestDataFactory.createTaskData(),
           title: longTitle
         };

         await expect(taskService.createTask(taskData))
           .rejects
           .toThrow('Title length exceeds maximum allowed');
       });

       it('should handle special characters and unicode in task data', async () => {
         const unicodeTitle = 'üöÄ Test Task with √©mojis and √ºnicode ‰∏≠Êñá';
         
         const taskData = {
           ...TestDataFactory.createTaskData(),
           title: unicodeTitle,
           description: 'Special chars: <script>alert("xss")</script>'
         };

         const task = await taskService.createTask(taskData);
         
         expect(task.title).toBe(unicodeTitle);
         expect(task.description).not.toContain('<script>'); // Should be sanitized
       });

       it('should handle date edge cases correctly', async () => {
         const taskData = {
           ...TestDataFactory.createTaskData(),
           dueDate: new Date('1970-01-01') // Unix epoch
         };

         await expect(taskService.createTask(taskData))
           .rejects
           .toThrow('Due date cannot be in the past');

         // Future date far ahead
         const farFuture = new Date('2099-12-31');
         const validTaskData = {
           ...TestDataFactory.createTaskData(),
           dueDate: farFuture
         };

         const task = await taskService.createTask(validTaskData);
         expect(task.dueDate).toEqual(farFuture);
       });
     });

     describe('Network and System Failure Simulation', () => {
       it('should handle repository connection failures', async () => {
         mockTaskRepository.create.mockRejectedValue(
           new Error('Database connection lost')
         );

         const taskData = TestDataFactory.createTaskData();

         await expect(taskService.createTask(taskData))
           .rejects
           .toThrow('Failed to create task due to system error');

         expect(mockLogger.error).toHaveBeenCalledWith(
           expect.stringContaining('Unexpected error'),
           expect.any(Object)
         );
       });

       it('should handle partial system failures gracefully', async () => {
         // Simulate notification service failure
         mockNotificationService.sendTaskCreatedNotification
           .mockRejectedValue(new Error('Notification service unavailable'));

         const taskData = TestDataFactory.createTaskData();
         
         // Task creation should still succeed
         const task = await taskService.createTask(taskData);
         expect(task).toBeDefined();

         // Should log the notification failure but not throw
         expect(mockLogger.warn).toHaveBeenCalledWith(
           expect.stringContaining('Notification failed'),
           expect.any(Object)
         );
       });
     });
   });
   ```

### Step 7: Generate API Testing Suite

1. **Create comprehensive API tests**:
   ```
   /tests Generate complete API test suite for TaskController including authentication, authorization, input validation, error responses, pagination, and rate limiting tests.
   ```

2. **Create** `src/__tests__/api/TaskController.api.test.ts`:
   ```typescript
   import request from 'supertest';
   import { TestDataFactory } from '../utils/testDataFactory';
   import { setupTestApp } from '../utils/testApp';

   describe('Task API Tests', () => {
     let app: Application;
     let authToken: string;

     beforeAll(async () => {
       app = await setupTestApp();
       authToken = await getAuthToken(app);
     });

     describe('GET /api/tasks', () => {
       it('should return paginated tasks with proper metadata', async () => {
         // Create test data
         await createTestTasks(10);

         const response = await request(app)
           .get('/api/tasks?page=1&limit=5')
           .set('Authorization', `Bearer ${authToken}`)
           .expect(200);

         expect(response.body).toMatchObject({
           data: expect.any(Array),
           meta: {
             page: 1,
             limit: 5,
             total: expect.any(Number),
             totalPages: expect.any(Number),
             hasNext: expect.any(Boolean),
             hasPrev: false
           }
         });

         expect(response.body.data).toHaveLength(5);
       });

       it('should filter tasks by status correctly', async () => {
         await createTestTasks(10, { status: TaskStatus.COMPLETED });
         await createTestTasks(5, { status: TaskStatus.PENDING });

         const response = await request(app)
           .get('/api/tasks?status=completed')
           .set('Authorization', `Bearer ${authToken}`)
           .expect(200);

         expect(response.body.data).toHaveLength(10);
         response.body.data.forEach(task => {
           expect(task.status).toBe(TaskStatus.COMPLETED);
         });
       });

       it('should return 401 for unauthenticated requests', async () => {
         await request(app)
           .get('/api/tasks')
           .expect(401);
       });
     });

     describe('POST /api/tasks', () => {
       it('should create task with valid data', async () => {
         const taskData = TestDataFactory.createTaskData();

         const response = await request(app)
           .post('/api/tasks')
           .set('Authorization', `Bearer ${authToken}`)
           .send(taskData)
           .expect(201);

         expect(response.body).toMatchObject({
           id: expect.any(String),
           title: taskData.title,
           description: taskData.description,
           status: TaskStatus.PENDING,
           createdAt: expect.any(String)
         });
       });

       it('should validate required fields', async () => {
         const invalidData = { description: 'Missing title' };

         const response = await request(app)
           .post('/api/tasks')
           .set('Authorization', `Bearer ${authToken}`)
           .send(invalidData)
           .expect(400);

         expect(response.body).toMatchObject({
           error: 'Validation Error',
           details: expect.arrayContaining([
             expect.objectContaining({
               field: 'title',
               message: expect.any(String)
             })
           ])
         });
       });
     });
   });
   ```

### Step 8: Generate Test Coverage Reports

1. **Run comprehensive test suite**:
   ```bash
   npm run test:coverage
   ```

2. **Analyze coverage report** and identify gaps

3. **Use Copilot to generate missing tests**:
   ```
   Based on the coverage report, I need tests for the following uncovered lines in TaskService. Generate tests to cover these specific scenarios: [paste uncovered line numbers and code].
   ```

### Step 9: Generate Load Testing

1. **Create load testing setup**:
   ```
   Create load tests for the task management API using a testing framework. Simulate realistic user behavior including creating tasks, updating status, and querying data under high concurrency.
   ```

2. **Create** `src/__tests__/load/api.load.test.ts`:
   ```typescript
   import { performance } from 'perf_hooks';
   import { TestDataFactory } from '../utils/testDataFactory';

   describe('Load Testing', () => {
     const CONCURRENT_USERS = 100;
     const OPERATIONS_PER_USER = 50;

     it('should handle high concurrent task creation', async () => {
       const startTime = performance.now();
       
       const userSimulations = Array.from({ length: CONCURRENT_USERS }, async () => {
         const operations = Array.from({ length: OPERATIONS_PER_USER }, async () => {
           const taskData = TestDataFactory.createTaskData();
           return await taskService.createTask(taskData);
         });
         
         return Promise.all(operations);
       });

       const results = await Promise.all(userSimulations);
       const endTime = performance.now();

       const totalOperations = CONCURRENT_USERS * OPERATIONS_PER_USER;
       const duration = endTime - startTime;
       const operationsPerSecond = totalOperations / (duration / 1000);

       console.log(`Completed ${totalOperations} operations in ${duration}ms`);
       console.log(`Throughput: ${operationsPerSecond} ops/sec`);

       // Performance assertions
       expect(operationsPerSecond).toBeGreaterThan(100); // At least 100 ops/sec
       expect(results.flat()).toHaveLength(totalOperations);
     });
   });
   ```

### Step 10: Automated Test Generation from Comments

1. **Add test-generating comments** to your methods:
   ```typescript
   /**
    * Assigns a task to a user with validation
    * @test Should validate user exists before assignment
    * @test Should check user permissions for task assignment
    * @test Should prevent assigning completed tasks
    * @test Should handle concurrent assignment attempts
    * @test Should log assignment events for audit trail
    */
   async assignTask(taskId: string, userId: string): Promise<void> {
     // Implementation...
   }
   ```

2. **Use Copilot to generate tests from comments**:
   ```
   Generate unit tests based on the @test comments in the assignTask method. Create comprehensive test cases that cover all mentioned scenarios with proper mocking and assertions.
   ```

## ‚úÖ Expected Results

After completing this phase, you should have:

### Comprehensive Test Suite:
- **Unit tests** with 90%+ code coverage
- **Integration tests** for complete workflows
- **API tests** with authentication and validation
- **Performance tests** with benchmarks
- **Edge case tests** for error conditions
- **Load tests** for scalability validation

### Test Quality Metrics:
```bash
# Coverage Report
Statements   : 95.2% ( 423/445 )
Branches     : 92.1% ( 156/169 )
Functions    : 96.7% ( 88/91 )
Lines        : 94.8% ( 400/422 )

# Performance Benchmarks
Task Creation: ~1000 ops/sec
Query Operations: ~5000 ops/sec
Bulk Operations: <5s for 1000 items
```

### Automated Test Practices:
- **Mock factories** for consistent test data
- **Test utilities** for common operations
- **Custom matchers** for domain-specific assertions
- **Performance monitoring** in test suites

## üí° Testing Best Practices with Copilot

1. **Write descriptive test names** that explain the scenario
2. **Use factories** for consistent test data generation
3. **Test edge cases** and error conditions thoroughly
4. **Mock external dependencies** properly
5. **Include performance assertions** for critical operations
6. **Generate tests incrementally** as you add features
7. **Review and customize** generated tests for accuracy

## üìö What You've Learned

- ‚úÖ Using Copilot to generate comprehensive test suites
- ‚úÖ Creating unit, integration, and performance tests
- ‚úÖ Implementing proper mocking strategies
- ‚úÖ Testing error conditions and edge cases
- ‚úÖ Building automated testing workflows
- ‚úÖ Generating realistic test data with factories
- ‚úÖ Measuring and improving test coverage

## üéØ Testing Command Reference

| Command | Purpose | Example Usage |
|---------|---------|---------------|
| `/tests` | Generate test cases | `/tests Create unit tests for TaskService` |
| `/fix` | Fix failing tests | `/fix Fix this test that's failing due to async issues` |
| `/explain` | Understand test code | `/explain How does this mock setup work?` |

## ‚û°Ô∏è Next Phase

Ready to create comprehensive documentation and diagrams?

[Continue to Phase 10 - Documentation & Diagrams ‚Üí](phase10-documentation-diagrams.md)

---

[üè† Back to Main README](../README.md)
